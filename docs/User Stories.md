# User Stories

---

## User Story 1: Initiate Scraper Build

*   **As a** Developer,
*   **I want to** submit a list of target URLs and a natural language objective describing the data I need via an API endpoint (`POST /builds`),
*   **So that** "The Brain" can start the process of intelligently configuring a scraper without me needing to specify selectors or tools.
*   **Requirements:**
    *   API endpoint accepts a JSON payload with `target_urls` (array of strings) and `user_objective` (string).
    *   Input validation for URL format and presence of objective.
    *   Authentication/Authorization check (API key).
    *   Generate a unique `build_id` for tracking.
    *   Trigger the backend LLM analysis and initial sample scraping process asynchronously.
    *   Return a `202 Accepted` response including the `build_id` and an initial status (e.g., `processing`).
*   **Edge Cases:**
    *   Invalid URL format in the list.
    *   Empty `target_urls` array or empty `user_objective`.
    *   Exceeding potential limits on URL count or objective length (if any).
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `202` with a valid `build_id`.
    *   A new build record is created in the system.
    *   The backend process to analyze and generate samples for this `build_id` is initiated.
    *   Subsequent `GET /builds/{build_id}` shows status `processing` or similar.
*   **Constraints:**
    *   API response time for this initial request should be fast (e.g., < 500ms).
    *   Clear limits (if any) on URL count/objective length should be documented.

---

**User Story 2: Review Initial Build Samples**

*   **As a** Developer,
*   **I want to** retrieve the initial data samples generated by "The Brain" for a specific build (`GET /builds/{build_id}`),
*   **So that** I can review the structure and content of the extracted data to verify if the API correctly understood my objective before confirming.
*   **Requirements:**
    *   API endpoint accepts `build_id` as a path parameter.
    *   Authentication/Authorization check.
    *   Return the current status of the build (`processing`, `pending_user_feedback`, `failed`, etc.).
    *   If status is `pending_user_feedback`, include a `package_results` field containing an array of sample JSON objects (one per sample URL processed).
*   **Edge Cases:**
    *   `build_id` not found (404).
    *   Build is still `processing` (return status, no samples yet).
    *   Build `failed` during sample generation (return status `failed` and error info).
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `200 OK`.
    *   If samples are ready, the response includes `status: pending_user_feedback` and a valid JSON array in `package_results` representing the extracted sample data.
*   **Constraints:**
    *   The number of sample URLs processed should be sufficient for review but limited to keep build time reasonable.
    *   The structure of `package_results` should be consistent.

---

**User Story 3: Provide Feedback for Build Refinement**

*   **As a** Developer,
*   **I want to** submit feedback (textual description of corrections or structured hints) on inaccurate initial samples via an API endpoint (`POST /builds/{build_id}/configure`),
*   **So that** "The Brain" can refine its scraping strategy, adjust tool selection, or modify its interpretation to produce more accurate results.
*   **Requirements:**
    *   API endpoint accepts `build_id` as a path parameter.
    *   Accepts a JSON payload containing `user_feedback` (string) and potentially optional `tool_hints` (structured object).
    *   Authentication/Authorization check.
    *   Validate that the build is in the `pending_user_feedback` state.
    *   Trigger the backend LLM refinement process asynchronously using the feedback.
    *   Update the build status (e.g., to `processing_feedback`).
    *   Return a `202 Accepted` response.
*   **Edge Cases:**
    *   `build_id` not found (404).
    *   Build is not in `pending_user_feedback` state (e.g., already confirmed or failed).
    *   Invalid feedback format.
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `202`.
    *   Build status is updated to reflect feedback processing.
    *   The backend refinement process is initiated, incorporating the provided feedback.
    *   Subsequent `GET /builds/{build_id}` will eventually transition back to `pending_user_feedback` (or `pending_final_confirmation`) with new samples, or `failed`.
*   **Constraints:**
    *   Clear documentation on the expected format for feedback and hints.
    *   Limits on the number of refinement cycles might be needed.

---

**User Story 4: Confirm scraper configuration**

*   **As a** Developer,
*   **I want to** confirm that the latest generated data samples for a build are correct and acceptable via an API endpoint (`POST /builds/{build_id}/confirm`),
*   **So that** the configuration used to generate those samples is saved and finalized, ready for executing full scrape runs.
*   **Requirements:**
    *   API endpoint accepts `build_id` as a path parameter.
    *   Authentication/Authorization check.
    *   Validate that the build is in a confirmable state (e.g., `pending_user_feedback`, `pending_final_confirmation`).
    *   Mark the build configuration associated with the latest successful samples as final (`final_configuration`).
    *   Update the build status to `confirmed`.
    *   Return a `200 OK` response, possibly including the `final_configuration` or just confirming success.
*   **Edge Cases:**
    *   `build_id` not found (404).
    *   Build is not in a confirmable state (e.g., still processing, failed, already confirmed).
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `200 OK`.
    *   The build status is updated to `confirmed`.
    *   The finalized scraper configuration is persistently stored and associated with the `build_id`.
*   **Constraints:**
    *   The `final_configuration` must contain all necessary information to execute a run independently later.

---

**User Story 5: Execute a Full Scrape Run**

*   **As a** Developer,
*   **I want to** initiate a full data extraction run using a previously confirmed build configuration (`build_id`) and a list of target URLs via an API endpoint (`POST /runs`),
*   **So that** I can efficiently scrape data from potentially many pages using the validated logic.
*   **Requirements:**
    *   API endpoint accepts a JSON payload with `build_id` (referencing a confirmed build) and `target_urls` (array of strings for this specific run).
    *   Authentication/Authorization check.
    *   Validate that the referenced `build_id` corresponds to a `confirmed` build.
    *   Generate a unique `run_id` for tracking this specific execution.
    *   Trigger the asynchronous scraping process for all provided `target_urls` using the `final_configuration` from the build.
    *   Return a `202 Accepted` response including the `run_id` and an initial status (e.g., `pending` or `running`).
*   **Edge Cases:**
    *   `build_id` not found or not in `confirmed` state.
    *   Invalid URL format in `target_urls`.
    *   Exceeding potential limits on URL count per run.
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `202` with a valid `run_id`.
    *   A new run record is created in the system.
    *   The backend process to scrape all target URLs is initiated.
    *   Subsequent `GET /runs/{run_id}` shows status `pending` or `running`.
*   **Constraints:**
    *   Clear limits on URL count per run (if any).
    *   Mechanisms to handle potentially long-running asynchronous tasks.

---

**User Story 6: Monitor Run Progress**

*   **As a** Developer,
*   **I want to** check the status and progress of an ongoing scrape run (`GET /runs/{run_id}`),
*   **So that** I can monitor its execution and estimate completion time or identify if it's stalled.
*   **Requirements:**
    *   API endpoint accepts `run_id` as a path parameter.
    *   Authentication/Authorization check.
    *   Return the current status (`pending`, `running`, `completed`, `failed`, `canceled`).
    *   If `running`, optionally include progress metrics (e.g., percentage complete, URLs processed/total).
*   **Edge Cases:**
    *   `run_id` not found (404).
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `200 OK` with the accurate, current status of the run.
    *   If running, progress metrics (if implemented) are returned.
*   **Constraints:**
    *   Status updates should be reasonably timely.
    *   API response time must be fast to support polling.
    *   Rate limiting on polling might be necessary.

---

**User Story 7: Retrieve Completed Run Results**

*   **As a** Developer,
*   **I want to** fetch the structured JSON results of a successfully completed scrape run (`GET /runs/{run_id}` or a dedicated `/runs/{run_id}/results`),
*   **So that** I can retrieve the extracted data for integration into my application or for analysis.
*   **Requirements:**
    *   API endpoint accepts `run_id`.
    *   Authentication/Authorization check.
    *   Validate that the run status is `completed`.
    *   Return the full array of structured JSON results, where each object corresponds to a successfully scraped target URL and contains the fields defined by the objective.
*   **Edge Cases:**
    *   `run_id` not found (404).
    *   Run status is not `completed` (e.g., `running`, `failed`). Return appropriate status/error.
    *   Very large result sets (consider pagination or alternative delivery).
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `200 OK`.
    *   The response body contains the complete and accurate JSON array of extracted data.
*   **Constraints:**
    *   Maximum API response payload size might necessitate pagination (`limit`, `offset` parameters) or alternative result delivery (e.g., webhook, cloud storage link).
    *   Results should be stored reliably for a defined period.

---

**User Story 8: Handle Build Configuration Failure**

*   **As a** Developer,
*   **I want to** retrieve clear error information when a scraper build process fails (`GET /builds/{build_id}`),
*   **So that** I can understand the reason for failure (e.g., inaccessible URLs, ambiguous objective, unsupported site structure) and attempt to correct my input for a retry.
*   **Requirements:**
    *   When a build fails during the `processing` or `processing_feedback` stages, its status should be updated to `failed`.
    *   The `GET /builds/{build_id}` endpoint for a failed build must return the `failed` status.
    *   The response must include an `error` field containing a descriptive message explaining the reason for failure.
*   **Edge Cases:**
    *   Failure due to LLM inability to understand objective.
    *   Failure due to target sites being universally inaccessible during sample run.
    *   Failure because no tool in the toolbox could extract meaningful data.
*   **Success Criteria:**
    *   `GET /builds/{build_id}` returns `200 OK` with `status: failed` and a non-empty, informative `error` message when a build has irrecoverably failed.
*   **Constraints:**
    *   Error messages should guide the user towards a solution without exposing internal system details.

---

**User Story 9: Handle Full Run Execution Failure**

*   **As a** Developer,
*   **I want to** retrieve clear error information when a full scrape run fails (`GET /runs/{run_id}`),
*   **So that** I can diagnose the problem (e.g., site changes broke the config, increased anti-scraping measures, tool errors) and decide on the next steps (retry, rebuild, report issue).
*   **Requirements:**
    *   When a run fails during execution, its status should be updated to `failed`.
    *   The `GET /runs/{run_id}` endpoint for a failed run must return the `failed` status.
    *   The response must include an `error` field detailing the failure reason(s). Consider if partial results for successfully scraped URLs should be provided.
*   **Edge Cases:**
    *   Run fails due to widespread blocking by target sites.
    *   Run fails due to significant layout changes on target sites invalidating the `final_configuration`.
    *   Internal errors within the scraping tools or infrastructure.
    *   Timeouts.
*   **Success Criteria:**
    *   `GET /runs/{run_id}` returns `200 OK` with `status: failed` and a non-empty, informative `error` message outlining the failure.
*   **Constraints:**
    *   Error reporting might need to distinguish between failures on individual URLs vs. catastrophic run failure.
    *   Policy on retaining partial results for failed runs.

---

**User Story 10: Cancel an In-Progress Scrape Run**

*   **As a** Developer,
*   **I want to** request the cancellation of a specific, currently running scrape (`POST /runs/{run_id}/cancel` or `DELETE /runs/{run_id}`),
*   **So that** I can stop unnecessary resource usage and data processing if I started the run by mistake or no longer need the results.
*   **Requirements:**
    *   API endpoint accepts `run_id`.
    *   Authentication/Authorization check.
    *   Validate that the run identified by `run_id` is currently in the `running` state.
    *   Send a termination signal to the backend process(es) handling the run.
    *   Return a `202 Accepted` response immediately.
    *   The backend should attempt graceful shutdown and cleanup.
    *   The run status should eventually be updated to `canceled` (polled via `GET /runs/{run_id}`).
*   **Edge Cases:**
    *   `run_id` not found (404).
    *   Run is not in `running` state (already completed, failed, etc.).
    *   Cancel request arrives just as the run finishes naturally.
    *   Authentication failure.
*   **Success Criteria:**
    *   API call returns `202 Accepted`.
    *   The backend initiates the cancellation procedure for the specified run.
    *   `GET /runs/{run_id}` eventually shows the status as `canceled`.
    *   Associated computing resources are released in a timely manner.
*   **Constraints:**
    *   Cancellation might not be instantaneous.
    *   Define system behavior if cancellation fails (e.g., does it transition to `failed`?).

## 11. User Story - Learn from Successful Scraper Build Confirmation

Okay, let's craft a user story specifically for "The Brain" feature where it learns from successful builds. This focuses on the *action* of saving the successful configuration to enable future learning.

---

**User Story: Learn from Successful Scraper Build Confirmation**

*   **As** The Brain (Configuration Engine),
*   **I want to** automatically save the complete, confirmed scraper package (`final_configuration`) and associated metadata (platform identifiers, output schema, objective keywords) to a dedicated knowledge base DB when a user confirms a build (`POST /builds/{build_id}/confirm`),
*   **So that** this validated configuration can be indexed and later retrieved as a high-quality example or starting point when analyzing new, similar build requests, improving future build accuracy and speed.

*   **Requirements:**
    *   **Trigger:** This process must activate automatically upon successful completion of a `POST /builds/{build_id}/confirm` request.
    *   **Data Capture:**
        *   Extract the `final_configuration` (containing selected tools, selectors, specific parameters, etc.) associated with the confirmed build.
        *   Extract or derive `platform_identifiers` from the build's target URLs (e.g., primary domain, maybe structural hashes of key page elements if feasible).
        *   Derive the `output_schema` from the structure of the successful JSON samples that were confirmed.
        *   Optionally extract keywords or generate embeddings from the original `user_objective`.
        *   Include the `build_id` for reference.
    *   **Storage:** Store the captured configuration and metadata in a dedicated database (e.g., combination of vector DB for semantic search and structured DB for metadata).
    *   **Indexing:** Ensure the stored data is appropriately indexed based on platform identifiers, schema structure, and objective semantics for efficient querying later.
    *   **Error Handling:** Handle potential failures during schema derivation or storage gracefully (e.g., log error but don't fail the user's confirmation request).

*   **Edge Cases:**
    *   Failure to derive a stable `output_schema` from the samples.
    *   Ambiguous or multiple `platform_identifiers` (e.g., URLs spanning different domains in one build).
    *   The exact same configuration (or very minor variations) being confirmed multiple times (consider updating usage count/timestamp vs. storing duplicates).
    *   Database write failures.

*   **Success Criteria / Acceptance Tests:**
    *   After a build is successfully confirmed via the API, a corresponding record containing the `final_configuration`, derived `platform_identifiers`, `output_schema`, and objective context exists in the knowledge base DB.
    *   The record is indexed and searchable based on platform, schema, and objective similarity (verifiable via direct DB query or a hypothetical internal search endpoint).
    *   The user's confirmation process is not significantly delayed by this background saving process.

*   **Constraints:**
    *   The process of deriving metadata and saving to the knowledge base should add minimal overhead to the build confirmation response time.
    *   Storage costs of the knowledge base must be considered.
    *   The chosen database(s) must support efficient querying based on the specified criteria (platform, schema, semantics).

*   **(Optional) Context:**
    *   This stored knowledge is the foundation for the *retrieval* part of the learning process. A separate (but tightly coupled) story would cover The Brain querying this DB *before* starting a new build analysis (`Upon receiving POST /builds, I want to query the knowledge base...`). This "saving" story enables that future retrieval and learning capability, representing the "experience" component of collective intelligence.

## 12. Uster Story - Leverage Past Build Knowledge for New Scraper Requests

**User Story: Leverage Past Build Knowledge for New Scraper Requests**

- **As** The Brain (Analysis Engine / Build Planner),
- **I want to** query the knowledge base DB for relevant, previously successful build configurations *before* starting the analysis for a new build request (`POST /builds`), using the new request's platform, objective, and potentially desired output structure as search criteria,
- **So that** I can leverage validated past solutions to accelerate the build process, improve accuracy, and potentially reuse or adapt proven configurations instead of starting from scratch every time.
- **Requirements:**
  - **Trigger:** This process must execute at the beginning of processing a new `POST /builds` request.
  - Querying:
    - Extract key information from the new request: platform identifiers (domain), objective keywords/embeddings, potentially implied output fields.
    - Formulate a query to the knowledge base DB searching for records with matching/similar platform, objective semantics, and output schema.
    - Define similarity thresholds and ranking logic for retrieved results.
  - Utilisation:
    - If a high-confidence match (e.g., same platform, very similar objective, matching schema) is found, potentially reuse the stored `final_configuration` directly or with minimal adaptation (*fast path*).
    - If relevant but not exact matches are found, use the retrieved configuration(s) as strong prior examples or context to guide the LLM analysis for the new build. This could involve including the past successful config in the prompt to the LLM.
    - If no relevant matches are found, proceed with the standard analysis process (using the LLM without specific prior examples).
  - **Logging:** Log whether a past configuration was consulted or reused for traceability and analysis of the feature's effectiveness.
- **Edge Cases:**
  - Multiple equally relevant past configurations found (how to choose?).
  - A retrieved configuration is known to be outdated due to recent site changes (requires a mechanism to potentially flag/deprecate old knowledge - future story?).
  - Querying the knowledge base significantly delays the start of the build process.
- **Success Criteria / Acceptance Tests:**
  - When a new build request closely matches a previously confirmed build in the knowledge base, the build process is demonstrably faster or directly reuses the configuration (measurable outcome).
  - When a new build request has similarities to past builds, the generated samples are more accurate on the first try compared to building without knowledge base consultation (qualitative/quantitative assessment).
  - Internal logs confirm that the knowledge base is being queried and results are being considered during the build analysis phase.
- **Constraints:**
  - The knowledge base query must be fast enough not to introduce unacceptable latency to the overall build start time.
  - The logic for determining similarity (platform, objective, schema) needs careful definition and tuning.
  - Requires the "Save Successful Build" story (the previous one) to be implemented first, as it populates the data source.